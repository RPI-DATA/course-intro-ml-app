{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"05_features_dummies.ipynb","version":"0.3.2","provenance":[{"file_id":"https://github.com/rpi-techfundamentals/spring2019-materials/blob/master/04-viz-api-scraper/05_features_dummies.ipynb","timestamp":1548905471532}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"xRlGzpOI8eO0","colab_type":"text"},"cell_type":"markdown","source":["\n","[![AnalyticsDojo](https://github.com/rpi-techfundamentals/spring2019-materials/blob/master/fig/final-logo.png?raw=1)](http://rpi.analyticsdojo.com)\n","<center><h1>Introduction to Feature Creation & Dummy Variables</h1></center>\n","<center><h3><a href = 'http://rpi.analyticsdojo.com'>rpi.analyticsdojo.com</a></h3></center>\n","\n"]},{"metadata":{"id":"g65Ns9748tPp","colab_type":"code","colab":{}},"cell_type":"code","source":["!wget https://raw.githubusercontent.com/rpi-techfundamentals/spring2019-materials/master/input/train.csv\n","!wget https://raw.githubusercontent.com/rpi-techfundamentals/spring2019-materials/master/input/test.csv"],"execution_count":0,"outputs":[]},{"metadata":{"id":"SOQxmELo8eO6","colab_type":"text"},"cell_type":"markdown","source":["# Feature Extraction"]},{"metadata":{"id":"ni94Qsmh8eO9","colab_type":"text"},"cell_type":"markdown","source":["Here we will talk about an important piece of machine learning: the extraction of\n","quantitative features from data.  By the end of this section you will\n","\n","- Know how features are extracted from real-world data.\n","- See an example of extracting numerical features from textual data\n","\n","In addition, we will go over several basic tools within scikit-learn which can be used to accomplish the above tasks."]},{"metadata":{"id":"bN81WgCT8ePB","colab_type":"text"},"cell_type":"markdown","source":["## What Are Features?"]},{"metadata":{"id":"EVeaLJJF8ePF","colab_type":"text"},"cell_type":"markdown","source":["### Numerical Features"]},{"metadata":{"id":"V9GFnaHe8ePJ","colab_type":"text"},"cell_type":"markdown","source":["Recall that data in scikit-learn is expected to be in two-dimensional arrays, of size\n","**n_samples** $\\times$ **n_features**.\n","\n","Previously, we looked at the iris dataset, which has 150 samples and 4 features"]},{"metadata":{"id":"MbAO_tXs8ePL","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.datasets import load_iris\n","iris = load_iris()\n","print(iris.data.shape)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HE_Zd6os8ePY","colab_type":"text"},"cell_type":"markdown","source":["These features are:\n","\n","- sepal length in cm\n","- sepal width in cm\n","- petal length in cm\n","- petal width in cm\n","\n","Numerical features such as these are pretty straightforward: each sample contains a list\n","of floating-point numbers corresponding to the features"]},{"metadata":{"id":"_NAt1est8ePb","colab_type":"text"},"cell_type":"markdown","source":["### Categorical Features"]},{"metadata":{"id":"1xPz1X_K8ePd","colab_type":"text"},"cell_type":"markdown","source":["What if you have categorical features?  For example, imagine there is data on the color of each\n","iris:\n","\n","    color in [red, blue, purple]\n","\n","You might be tempted to assign numbers to these features, i.e. *red=1, blue=2, purple=3*\n","but in general **this is a bad idea**.  Estimators tend to operate under the assumption that\n","numerical features lie on some continuous scale, so, for example, 1 and 2 are more alike\n","than 1 and 3, and this is often not the case for categorical features.\n","\n","In fact, the example above is a subcategory of \"categorical\" features, namely, \"nominal\" features. Nominal features don't imply an order, whereas \"ordinal\" features are categorical features that do imply an order. An example of ordinal features would be T-shirt sizes, e.g., XL > L > M > S. \n","\n","One work-around for parsing nominal features into a format that prevents the classification algorithm from asserting an order is the so-called one-hot encoding representation. Here, we give each category its own dimension.  \n","\n","The enriched iris feature set would hence be in this case:\n","\n","- sepal length in cm\n","- sepal width in cm\n","- petal length in cm\n","- petal width in cm\n","- color=purple (1.0 or 0.0)\n","- color=blue (1.0 or 0.0)\n","- color=red (1.0 or 0.0)\n","\n","Note that using many of these categorical features may result in data which is better\n","represented as a **sparse matrix**, as we'll see with the text classification example\n","below."]},{"metadata":{"id":"E6REBABX8ePe","colab_type":"text"},"cell_type":"markdown","source":["### Derived Features"]},{"metadata":{"id":"qyDGjGgv8ePi","colab_type":"text"},"cell_type":"markdown","source":["Another common feature type are **derived features**, where some pre-processing step is\n","applied to the data to generate features that are somehow more informative.  Derived\n","features may be based in **feature extraction** and **dimensionality reduction** (such as PCA or manifold learning),\n","may be linear or nonlinear combinations of features (such as in polynomial regression),\n","or may be some more sophisticated transform of the features."]},{"metadata":{"id":"s_fwMkb68ePj","colab_type":"text"},"cell_type":"markdown","source":["### Combining Numerical and Categorical Features"]},{"metadata":{"id":"7pW1UhJT8ePk","colab_type":"text"},"cell_type":"markdown","source":["As an example of how to work with both categorical and numerical data, we will perform survival predicition for the passengers of the HMS Titanic.\n"]},{"metadata":{"id":"bvj3Wids8ePm","colab_type":"code","colab":{}},"cell_type":"code","source":["import os\n","import pandas as pd\n","titanic = pd.read_csv('train.csv')\n","print(titanic.columns)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0xqjk2-P8ePp","colab_type":"text"},"cell_type":"markdown","source":["Here is a broad description of the keys and what they mean:\n","\n","```\n","pclass          Passenger Class\n","                (1 = 1st; 2 = 2nd; 3 = 3rd)\n","survival        Survival\n","                (0 = No; 1 = Yes)\n","name            Name\n","sex             Sex\n","age             Age\n","sibsp           Number of Siblings/Spouses Aboard\n","parch           Number of Parents/Children Aboard\n","ticket          Ticket Number\n","fare            Passenger Fare\n","cabin           Cabin\n","embarked        Port of Embarkation\n","                (C = Cherbourg; Q = Queenstown; S = Southampton)\n","boat            Lifeboat\n","body            Body Identification Number\n","home.dest       Home/Destination\n","```\n","\n","In general, it looks like `name`, `sex`, `cabin`, `embarked`, `boat`, `body`, and `homedest` may be candidates for categorical features, while the rest appear to be numerical features. We can also look at the first couple of rows in the dataset to get a better understanding:"]},{"metadata":{"id":"bqmMR9G78ePr","colab_type":"code","colab":{}},"cell_type":"code","source":["titanic.head()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"54WY6zD78ePv","colab_type":"text"},"cell_type":"markdown","source":["We clearly want to discard the \"boat\" and \"body\" columns for any classification into survived vs not survived as they already contain this information. The name is unique to each person (probably) and also non-informative. For a first try, we will use \"pclass\", \"sibsp\", \"parch\", \"fare\" and \"embarked\" as our features:"]},{"metadata":{"id":"EDYT-WYR8ePv","colab_type":"code","colab":{}},"cell_type":"code","source":["labels = titanic.Survived.values\n","features = titanic[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"6hL63MtX8ePz","colab_type":"code","colab":{}},"cell_type":"code","source":["features.head()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"uzE0gEFz8eP3","colab_type":"text"},"cell_type":"markdown","source":["The data now contains only useful features, but they are not in a format that the machine learning algorithms can understand. We need to transform the strings \"male\" and \"female\" into binary variables that indicate the gender, and similarly for \"embarked\".\n","We can do that using the pandas ``get_dummies`` function:"]},{"metadata":{"id":"ehWm5zgs8eP4","colab_type":"code","colab":{}},"cell_type":"code","source":["featuremodel=pd.get_dummies(features)\n","featuremodel"],"execution_count":0,"outputs":[]},{"metadata":{"id":"sm8tB4Zq8eP7","colab_type":"text"},"cell_type":"markdown","source":["Notice that this includes N dummy variables.  When we are modeling we will need N-1 categorical variables. "]},{"metadata":{"id":"RqsW2ClZ8eP7","colab_type":"code","colab":{}},"cell_type":"code","source":["pd.get_dummies(features, drop_first=True).head()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"6OrnBmC28eP9","colab_type":"text"},"cell_type":"markdown","source":["This transformation successfully encoded the string columns. However, one might argue that the class is also a categorical variable. We can explicitly list the columns to encode using the ``columns`` parameter, and include ``pclass``:"]},{"metadata":{"id":"FBnqwOjE8eP-","colab_type":"code","colab":{}},"cell_type":"code","source":["features_dummies = pd.get_dummies(features, columns=['Pclass', 'Sex', 'Embarked'], drop_first=True)\n","features_dummies"],"execution_count":0,"outputs":[]},{"metadata":{"id":"8F51z9NI8eQA","colab_type":"code","colab":{}},"cell_type":"code","source":["#Transform from Pandas to numpy with .values\n","data = features_dummies.values"],"execution_count":0,"outputs":[]},{"metadata":{"id":"44AviIcw8eQD","colab_type":"code","colab":{}},"cell_type":"code","source":["data"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4pUL_7hg9QR5","colab_type":"code","colab":{}},"cell_type":"code","source":["type(data)"],"execution_count":0,"outputs":[]}]}