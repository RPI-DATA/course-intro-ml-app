{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01-Neural-Networks.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "Z6yF_rLV5Q4L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Neural Networks and the Simplist XOR Problem\n",
        "- This was adopted from the PyTorch Tutorials. \n",
        "- Simple supervised machine learning.\n",
        "- http://pytorch.org/tutorials/beginner/pytorch_with_examples.html"
      ]
    },
    {
      "metadata": {
        "id": "8N8bijcQ5Q4Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Neural Networks \n",
        "- Neural networks are the foundation of deep learning, which has revolutionized the \n",
        "\n",
        "```In the mathematical theory of artificial neural networks, the universal approximation theorem states[1] that a feed-forward network with a single hidden layer containing a finite number of neurons (i.e., a multilayer perceptron), can approximate continuous functions on compact subsets of Rn, under mild assumptions on the activation function.```\n",
        "\n",
        "- A simple task that Neural Networks can do but simple linear models cannot is called the [XOR problem](https://medium.com/@jayeshbahire/the-xor-problem-in-neural-networks-50006411840b).\n",
        "\n",
        "- The XOR problem involves an output being 1 if either of two inputs is 1, but not both. "
      ]
    },
    {
      "metadata": {
        "id": "lK5CNxwO5Q4T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Generate Fake Data\n",
        "- `D_in` is the number of dimensions of an input varaible.\n",
        "- `D_out` is the number of dimentions of an output variable.\n",
        "- Here we are learning some special \"fake\" data that represents the xor problem. \n",
        "- Here, the dv is 1 if either the first or second variable is \n"
      ]
    },
    {
      "metadata": {
        "id": "59Pm-EXK5Q4W",
        "colab_type": "code",
        "outputId": "1d59412e-c93b-41c3-a82b-5485d8c03850",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        }
      },
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import numpy as np\n",
        "\n",
        "#This is out input array. \n",
        "X = np.array([ [0,0,0],[1,0,0],[0,1,0],[0,0,0] ])\n",
        "y = np.array([[0,1,1,0]]).T\n",
        "print(\"Input data:\\n\",X,\"\\n Output data:\\n\",y)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:\n",
            " [[0 0 0]\n",
            " [1 0 0]\n",
            " [0 1 0]\n",
            " [0 0 0]] \n",
            " Output data:\n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Cx3UZLcC5Q4m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### A Simple Neural Network \n",
        "- Here we are going to build a neural network. \n",
        "- First layer (`D_in`)has to be the length of the input.\n",
        "- `H` is the length of the output.\n",
        "-  `D_out` is 1 as it will be the probability it is a 1."
      ]
    },
    {
      "metadata": {
        "id": "JtPFUCoR5Q4n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "D_in = 3 \n",
        "H = 3\n",
        "d_out = 3\n",
        "# Randomly initialize weights\n",
        "w1 = np.random.randn(D_in, H)\n",
        "w2 = np.random.randn(H, D_out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hYXsVn3z8AR-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### But \"Hidden Layers\" Aren't Hidden\n",
        "- Let's take a look \n",
        "- These are just random numbers."
      ]
    },
    {
      "metadata": {
        "id": "BxVP_1eI62BT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "60c84eb1-1476-4cd3-facf-96f13677ca78"
      },
      "cell_type": "code",
      "source": [
        "print(w1, w2)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.0250052   0.39248636 -0.95856533]\n",
            " [-0.54775758  1.01757558  0.0834066 ]\n",
            " [ 0.00108387  0.20093661  0.49683823]] [[-0.68675167]\n",
            " [-0.49060388]\n",
            " [ 0.3668211 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_fk9EaZ89TGb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Update the Weights using Gradient Decent\n",
        "- Calculate the predited value\n",
        "- Calculate the loss function\n",
        "- Compute the gradients of w1 and w2 with respect to the loss function\n",
        "- Update the weights using the learning rate "
      ]
    },
    {
      "metadata": {
        "id": "oEn83e_t5Q40",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "learning_rate = 1e-3  #Sets how fast our updates will occur\n",
        "for t in range(300):\n",
        "    # Forward pass: compute predicted y\n",
        "    h = x.dot(w1)\n",
        "    \n",
        "    #A relu is just the activation.\n",
        "    h_relu = np.maximum(h, 0)\n",
        "    y_pred = h_relu.dot(w2)\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = np.square(y_pred - y).sum()\n",
        "    print(t, loss)\n",
        "\n",
        "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
        "    grad_y_pred = 2.0 * (y_pred - y)\n",
        "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
        "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
        "    grad_h = grad_h_relu.copy()\n",
        "    grad_h[h < 0] = 0 ## Relu just removes the negative numbers. We will talk about it later.\n",
        "    grad_w1 = x.T.dot(grad_h)\n",
        "\n",
        "    # Update weights\n",
        "    w1 -= learning_rate * grad_w1\n",
        "    w2 -= learning_rate * grad_w2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tMzDX7Qj5Q5T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Fully connected "
      ]
    },
    {
      "metadata": {
        "id": "krRvrLfM-LTy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Verify the Predictions \n",
        "- Obtained a predicted value from our model and compare to origional. "
      ]
    },
    {
      "metadata": {
        "id": "tUWp2FU15Q5v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "y_pred = h_relu.dot(w2)\n",
        "y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uWNhz_PI5Q52",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JMvnoVX55Q56",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#We can see that the weights have been updated. \n",
        "w1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0SIWdk6U5Q5_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "w2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HAZjdYBF5Q6C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Relu just removes the negative numbers.  \n",
        "h_relu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wJ3UE4Cy5Q6F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}