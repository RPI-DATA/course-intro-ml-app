{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![AnalyticsDojo](../fig/final-logo.png)](http://rpi.analyticsdojo.com)\n",
    "<center><h1>Pytorch with the MNIST Dataset - MINST</h1></center>\n",
    "<center><h3><a href = 'http://rpi.analyticsdojo.com'>rpi.analyticsdojo.com</a></h3></center>\n",
    "\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/rpi-techfundamentals/fall2018-materials/blob/master/10-deep-learning/04-pytorch-mnist.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "From Kaggle: \n",
    "\"MNIST (\"Modified National Institute of Standards and Technology\") is the de facto “hello world” dataset of computer vision. Since its release in 1999, this classic dataset of handwritten images has served as the basis for benchmarking classification algorithms. As new machine learning techniques emerge, MNIST remains a reliable resource for researchers and learners alike.\"\n",
    "\n",
    "[Read more.](https://www.kaggle.com/c/digit-recognizer)\n",
    "\n",
    "\n",
    "<a title=\"By Josef Steppan [CC BY-SA 4.0 (https://creativecommons.org/licenses/by-sa/4.0)], from Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:MnistExamples.png\"><img width=\"512\" alt=\"MnistExamples\" src=\"https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is adopted from the pytorch examples repository. \n",
    "It is licensed under BSD 3-Clause \"New\" or \"Revised\" License.\n",
    "Source: https://github.com/pytorch/examples/\n",
    "LICENSE: https://github.com/pytorch/examples/blob/master/LICENSE\n",
    "\n",
    "![](mnist-comparison.png)\n",
    "Table from [Wikipedia](https://en.wikipedia.org/wiki/MNIST_database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /anaconda3/envs/carme/lib/python3.6/site-packages (0.3.0.post4)\n",
      "Requirement already satisfied: torchvision in /anaconda3/envs/carme/lib/python3.6/site-packages (0.2.0)\n",
      "Requirement already satisfied: pyyaml in /anaconda3/envs/carme/lib/python3.6/site-packages (from torch) (3.12)\n",
      "Requirement already satisfied: numpy in /anaconda3/envs/carme/lib/python3.6/site-packages (from torch) (1.12.1)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /anaconda3/envs/carme/lib/python3.6/site-packages (from torchvision) (5.1.0)\n",
      "Requirement already satisfied: six in /anaconda3/envs/carme/lib/python3.6/site-packages (from torchvision) (1.11.0)\n",
      "\u001b[31mkaggle-cli 0.12.13 has requirement lxml<4.1,>=4.0.0, but you'll have lxml 3.8.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mawscli 1.14.32 has requirement botocore==1.8.36, but you'll have botocore 1.9.7 which is incompatible.\u001b[0m\n",
      "\u001b[31mapache-airflow 1.9.0 has requirement bleach==2.1.2, but you'll have bleach 2.1.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mapache-airflow 1.9.0 has requirement flask<0.12,>=0.11, but you'll have flask 0.12.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mapache-airflow 1.9.0 has requirement jinja2<2.9.0,>=2.7.3, but you'll have jinja2 2.10 which is incompatible.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch Advantages vs Tensorflow\n",
    "- Pytorch Enables dynamic computational graphs (which change be changed) while Tensorflow is static. \n",
    "- Tensorflow enables easier deployment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "\n",
    "\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args={}\n",
    "kwargs={}\n",
    "args['batch_size']=1000\n",
    "args['test_batch_size']=1000\n",
    "args['epochs']=10  #The number of Epochs is the number of times you go through the full dataset. \n",
    "args['lr']=0.01 #Learning rate is how fast it will decend. \n",
    "args['momentum']=0.5 #SGD momentum (default: 0.5) Momentum is a moving average of our gradients (helps to keep direction).\n",
    "\n",
    "args['seed']=1 #random seed\n",
    "args['log_interval']=10\n",
    "args['cuda']=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#load the data\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args['batch_size'], shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args['test_batch_size'], shuffle=True, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    #This defines the structure of the NN.\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()  #Dropout\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #Convolutional Layer/Pooling Layer/Activation\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2)) \n",
    "        #Convolutional Layer/Dropout/Pooling Layer/Activation\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        #Fully Connected Layer/Activation\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        #Fully Connected Layer/Activation\n",
    "        x = self.fc2(x)\n",
    "        #Softmax gets probabilities. \n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if args['cuda']:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        #Variables in Pytorch are differenciable. \n",
    "        data, target = Variable(data), Variable(target)\n",
    "        #This will zero out the gradients for this batch. \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        # Calculate the loss The negative log likelihood loss. It is useful to train a classification problem with C classes.\n",
    "        loss = F.nll_loss(output, target)\n",
    "        #dloss/dx for every Variable \n",
    "        loss.backward()\n",
    "        #to do a one-step update on our parameter.\n",
    "        optimizer.step()\n",
    "        #Print out the loss periodically. \n",
    "        if batch_idx % args['log_interval'] == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        if args['cuda']:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.338192\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 2.305725\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 2.289212\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 2.283156\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 2.270567\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 2.261630\n",
      "\n",
      "Test set: Average loss: 2.2199, Accuracy: 3655/10000 (37%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.240778\n",
      "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 2.209014\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 2.170792\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 2.140490\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 2.080513\n",
      "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 1.990342\n",
      "\n",
      "Test set: Average loss: 1.7368, Accuracy: 7205/10000 (72%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 1.924992\n",
      "Train Epoch: 3 [10000/60000 (17%)]\tLoss: 1.759480\n",
      "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 1.636611\n",
      "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 1.517218\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 1.348585\n",
      "Train Epoch: 3 [50000/60000 (83%)]\tLoss: 1.313530\n",
      "\n",
      "Test set: Average loss: 0.8124, Accuracy: 8438/10000 (84%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 1.169621\n",
      "Train Epoch: 4 [10000/60000 (17%)]\tLoss: 1.145530\n",
      "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 1.056403\n",
      "Train Epoch: 4 [30000/60000 (50%)]\tLoss: 0.992876\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 0.980686\n",
      "Train Epoch: 4 [50000/60000 (83%)]\tLoss: 0.950357\n",
      "\n",
      "Test set: Average loss: 0.5138, Accuracy: 8800/10000 (88%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.930668\n",
      "Train Epoch: 5 [10000/60000 (17%)]\tLoss: 0.879105\n",
      "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 0.874244\n",
      "Train Epoch: 5 [30000/60000 (50%)]\tLoss: 0.787681\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 0.814346\n",
      "Train Epoch: 5 [50000/60000 (83%)]\tLoss: 0.779896\n",
      "\n",
      "Test set: Average loss: 0.4082, Accuracy: 8966/10000 (90%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.744148\n",
      "Train Epoch: 6 [10000/60000 (17%)]\tLoss: 0.730266\n",
      "Train Epoch: 6 [20000/60000 (33%)]\tLoss: 0.730913\n",
      "Train Epoch: 6 [30000/60000 (50%)]\tLoss: 0.697980\n",
      "Train Epoch: 6 [40000/60000 (67%)]\tLoss: 0.736012\n",
      "Train Epoch: 6 [50000/60000 (83%)]\tLoss: 0.711165\n",
      "\n",
      "Test set: Average loss: 0.3525, Accuracy: 9069/10000 (91%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.722657\n",
      "Train Epoch: 7 [10000/60000 (17%)]\tLoss: 0.652839\n",
      "Train Epoch: 7 [20000/60000 (33%)]\tLoss: 0.716362\n",
      "Train Epoch: 7 [30000/60000 (50%)]\tLoss: 0.678424\n",
      "Train Epoch: 7 [40000/60000 (67%)]\tLoss: 0.665473\n",
      "Train Epoch: 7 [50000/60000 (83%)]\tLoss: 0.614177\n",
      "\n",
      "Test set: Average loss: 0.3153, Accuracy: 9121/10000 (91%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.621331\n",
      "Train Epoch: 8 [10000/60000 (17%)]\tLoss: 0.550397\n",
      "Train Epoch: 8 [20000/60000 (33%)]\tLoss: 0.623889\n",
      "Train Epoch: 8 [30000/60000 (50%)]\tLoss: 0.609498\n",
      "Train Epoch: 8 [40000/60000 (67%)]\tLoss: 0.632714\n",
      "Train Epoch: 8 [50000/60000 (83%)]\tLoss: 0.567455\n",
      "\n",
      "Test set: Average loss: 0.2897, Accuracy: 9188/10000 (92%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.637325\n",
      "Train Epoch: 9 [10000/60000 (17%)]\tLoss: 0.607037\n",
      "Train Epoch: 9 [20000/60000 (33%)]\tLoss: 0.607436\n",
      "Train Epoch: 9 [30000/60000 (50%)]\tLoss: 0.605397\n",
      "Train Epoch: 9 [40000/60000 (67%)]\tLoss: 0.540220\n",
      "Train Epoch: 9 [50000/60000 (83%)]\tLoss: 0.567621\n",
      "\n",
      "Test set: Average loss: 0.2713, Accuracy: 9224/10000 (92%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.538887\n",
      "Train Epoch: 10 [10000/60000 (17%)]\tLoss: 0.529944\n",
      "Train Epoch: 10 [20000/60000 (33%)]\tLoss: 0.570023\n",
      "Train Epoch: 10 [30000/60000 (50%)]\tLoss: 0.558310\n",
      "Train Epoch: 10 [40000/60000 (67%)]\tLoss: 0.513574\n",
      "Train Epoch: 10 [50000/60000 (83%)]\tLoss: 0.528905\n",
      "\n",
      "Test set: Average loss: 0.2524, Accuracy: 9284/10000 (93%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "if args['cuda']:\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=args['lr'], momentum=args['momentum'])\n",
    "\n",
    "for epoch in range(1, args['epochs'] + 1):\n",
    "    train(epoch)\n",
    "    test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
