{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[![AnalyticsDojo](../fig/final-logo.png)](http://rpi.analyticsdojo.com)\n",
    "<center><h1>Intro to Tensorflow - MINST</h1></center>\n",
    "<center><h3><a href = 'http://rpi.analyticsdojo.com'>rpi.analyticsdojo.com</a></h3></center>\n",
    "\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/rpi-techfundamentals/fall2018-materials/blob/master/10-deep-learning/06-tensorflow-minst.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "Adopted from [Hands-On Machine Learning with Scikit-Learn and TensorFlow by Aurélien Géron](https://github.com/ageron/handson-ml).\n",
    "\n",
    "\n",
    "Apache License\n",
    "Version 2.0, January 2004\n",
    "http://www.apache.org/licenses/\n",
    "TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n",
    "\n",
    "[For full license see repository.](https://github.com/ageron/handson-ml/blob/master/LICENSE)\n",
    "\n",
    "\n",
    "**Chapter 10 – Introduction to Artificial Neural Networks**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_This notebook contains all the sample code and solutions to the exercices in chapter 10._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's make sure this notebook works well in both python 2 and 3, import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \"/home/jovyan/techfundamentals-fall2017-materials/classes/13-deep-learning\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, 'images', fig_id + \".png\")\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST\n",
    "- Very common machine learning library with goal to classify digits. \n",
    "- This example is using MNIST handwritten digits, which contains 55,000 examples for training and 10,000 examples for testing. The digits have been size-normalized and centered in a fixed-size image (28x28 pixels) with values from 0 to 1. For simplicity, each image has been flattened and converted to a 1-D numpy array of 784 features (28*28).\n",
    "![MNIST Dataset](http://neuralnetworksanddeeplearning.com/images/mnist_100_digits.png)\n",
    "\n",
    "More info: http://yann.lecun.com/exdb/mnist/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "Training set:  (55000, 784) \n",
      "Test set:  (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "X_train = mnist.train.images\n",
    "X_test = mnist.test.images\n",
    "y_train = mnist.train.labels.astype(\"int\")\n",
    "y_test = mnist.test.labels.astype(\"int\")\n",
    "print (\"Training set: \", X_train.shape,\"\\nTest set: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.38039219  0.37647063\n",
      "   0.3019608   0.46274513  0.2392157   0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.          0.35294119  0.5411765   0.92156869\n",
      "   0.92156869  0.92156869  0.92156869  0.92156869  0.92156869  0.98431379\n",
      "   0.98431379  0.97254908  0.99607849  0.96078438  0.92156869  0.74509805\n",
      "   0.08235294  0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.          0.54901963  0.98431379  0.99607849  0.99607849\n",
      "   0.99607849  0.99607849  0.99607849  0.99607849  0.99607849  0.99607849\n",
      "   0.99607849  0.99607849  0.99607849  0.99607849  0.99607849  0.99607849\n",
      "   0.74117649  0.09019608  0.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.          0.88627458  0.99607849  0.81568635  0.78039223\n",
      "   0.78039223  0.78039223  0.78039223  0.54509807  0.2392157   0.2392157\n",
      "   0.2392157   0.2392157   0.2392157   0.50196081  0.8705883   0.99607849\n",
      "   0.99607849  0.74117649  0.08235294  0.          0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.          0.14901961  0.32156864  0.0509804   0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.13333334  0.83529419  0.99607849  0.99607849\n",
      "   0.45098042  0.          0.          0.          0.          0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.32941177  0.99607849  0.99607849\n",
      "   0.91764712  0.          0.          0.          0.          0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.32941177  0.99607849  0.99607849\n",
      "   0.91764712  0.          0.          0.          0.          0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.41568631  0.6156863   0.99607849  0.99607849\n",
      "   0.95294124  0.20000002  0.          0.          0.          0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.09803922  0.45882356\n",
      "   0.89411771  0.89411771  0.89411771  0.99215692  0.99607849  0.99607849\n",
      "   0.99607849  0.99607849  0.94117653  0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.26666668  0.4666667   0.86274517  0.99607849\n",
      "   0.99607849  0.99607849  0.99607849  0.99607849  0.99607849  0.99607849\n",
      "   0.99607849  0.99607849  0.55686277  0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.14509805  0.73333335  0.99215692  0.99607849  0.99607849  0.99607849\n",
      "   0.87450987  0.80784321  0.80784321  0.29411766  0.26666668  0.84313732\n",
      "   0.99607849  0.99607849  0.45882356  0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.44313729  0.8588236   0.99607849  0.94901967  0.89019614  0.45098042\n",
      "   0.34901962  0.12156864  0.          0.          0.          0.\n",
      "   0.7843138   0.99607849  0.9450981   0.16078432  0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.66274512  0.99607849  0.6901961   0.24313727  0.          0.          0.\n",
      "   0.          0.          0.          0.          0.18823531  0.90588242\n",
      "   0.99607849  0.91764712  0.          0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.07058824  0.48627454  0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.32941177  0.99607849\n",
      "   0.99607849  0.65098041  0.          0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.54509807  0.99607849  0.9333334\n",
      "   0.22352943  0.          0.          0.          0.          0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.82352948  0.98039222  0.99607849  0.65882355\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.94901967  0.99607849  0.93725497  0.22352943\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.34901962  0.98431379  0.9450981   0.33725491  0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.01960784  0.80784321  0.96470594  0.6156863   0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.01568628  0.45882356  0.27058825  0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.        ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD/CAYAAADxA2MgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x103139240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.12156864  0.51764709  0.99607849  0.99215692  0.99607849  0.83529419\n",
      "   0.32156864  0.          0.          0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.08235294  0.55686277\n",
      "   0.91372555  0.98823535  0.99215692  0.98823535  0.99215692  0.98823535\n",
      "   0.87450987  0.07843138  0.          0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.48235297  0.99607849  0.99215692\n",
      "   0.99607849  0.99215692  0.87843144  0.7960785   0.7960785   0.87450987\n",
      "   1.          0.83529419  0.          0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.7960785   0.99215692  0.98823535\n",
      "   0.99215692  0.83137262  0.07843138  0.          0.          0.2392157\n",
      "   0.99215692  0.98823535  0.          0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.16078432  0.95294124  0.87843144  0.7960785\n",
      "   0.71764708  0.16078432  0.59607846  0.11764707  0.          0.          1.\n",
      "   0.99215692  0.40000004  0.          0.          0.          0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.15686275  0.07843138  0.          0.\n",
      "   0.40000004  0.99215692  0.19607845  0.          0.32156864  0.99215692\n",
      "   0.98823535  0.07843138  0.          0.          0.          0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.32156864  0.83921576  0.12156864  0.44313729  0.91372555  0.99607849\n",
      "   0.91372555  0.          0.          0.          0.          0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.24313727  0.40000004\n",
      "   0.32156864  0.16078432  0.99215692  0.90980399  0.99215692  0.98823535\n",
      "   0.91372555  0.19607845  0.          0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.59607846  0.99215692\n",
      "   0.99607849  0.99215692  0.99607849  0.99215692  0.99607849  0.91372555\n",
      "   0.48235297  0.          0.          0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.59607846  0.98823535\n",
      "   0.99215692  0.98823535  0.99215692  0.98823535  0.75294125  0.19607845\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.24313727  0.71764708\n",
      "   0.7960785   0.95294124  0.99607849  0.99215692  0.24313727  0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.15686275  0.67450982  0.98823535  0.7960785   0.07843138  0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.08235294  0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.71764708  0.99607849  0.43921572\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.24313727\n",
      "   0.7960785   0.63921571  0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.2392157   0.99215692  0.59215689\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.08235294  0.83921576\n",
      "   0.75294125  0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.04313726  0.83529419  0.99607849  0.59215689\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.40000004  0.99215692\n",
      "   0.59215689  0.          0.          0.          0.          0.          0.\n",
      "   0.          0.16078432  0.83529419  0.98823535  0.99215692  0.43529415\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.16078432  1.\n",
      "   0.83529419  0.36078432  0.20000002  0.          0.          0.12156864\n",
      "   0.36078432  0.67843139  0.99215692  0.99607849  0.99215692  0.55686277\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.67450982\n",
      "   0.98823535  0.99215692  0.98823535  0.7960785   0.7960785   0.91372555\n",
      "   0.98823535  0.99215692  0.98823535  0.99215692  0.50980395  0.07843138\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.08235294\n",
      "   0.7960785   1.          0.99215692  0.99607849  0.99215692  0.99607849\n",
      "   0.99215692  0.95686281  0.7960785   0.32156864  0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.07843138  0.59215689  0.59215689  0.99215692  0.67058825  0.59215689\n",
      "   0.59215689  0.15686275  0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.        ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD/CAYAAADxA2MgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11fd601d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# List a few images and print the data to get a feel for it. \n",
    "images = 2\n",
    "for i in range(images):\n",
    "    #Reshape \n",
    "    x=np.reshape(X_train[i], [28, 28])\n",
    "    print(x)\n",
    "    plt.imshow(x, cmap=plt.get_cmap('gray_r'))\n",
    "    plt.show()\n",
    "#    print(\"Model prediction:\", preds[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFLearn: Deep learning library featuring a higher-level API for TensorFlow\n",
    "\n",
    "- TFlearn is a modular and transparent deep learning library built on top of Tensorflow. \n",
    "- It was designed to provide a higher-level API to TensorFlow in order to facilitate and speed-up experimentations\n",
    "- Fully transparent and compatible with Tensorflow\n",
    "- [DNN classifier](https://www.tensorflow.org/api_docs/python/tf/contrib/learn/DNNClassifier)\n",
    "- `hidden_units` list of hidden units per layer. All layers are fully connected. Ex. [64, 32] means first layer has 64 nodes and second one has 32.\n",
    "- [Scikit learn wrapper for TensorFlow Learn Estimator](https://www.tensorflow.org/api_docs/python/tf/contrib/learn/SKCompat)\n",
    "- See [tflearn documentation](http://tflearn.org/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/2l/hz84cqd11f1_kgngygpjvbmh0000gn/T/tmp9ryubfoi\n",
      "INFO:tensorflow:Using config: {'_task_id': 0, '_is_chief': True, '_save_checkpoints_steps': None, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_save_summary_steps': 100, '_master': '', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x11fc21ef0>, '_task_type': None, '_num_ps_replicas': 0, '_environment': 'local', '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_save_checkpoints_secs': 600, '_tf_random_seed': 42}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.contrib.learn' has no attribute 'SKCompat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5b9939f7137a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m dnn_clf = tf.contrib.learn.DNNClassifier(hidden_units=[300,100], n_classes=10,\n\u001b[1;32m      6\u001b[0m                                          feature_columns=feature_cols, config=config)\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdnn_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSKCompat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdnn_clf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# if TensorFlow >= 1.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mdnn_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.contrib.learn' has no attribute 'SKCompat'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "config = tf.contrib.learn.RunConfig(tf_random_seed=42) # not shown in the config\n",
    "feature_cols = tf.contrib.learn.infer_real_valued_columns_from_input(X_train)\n",
    "# List of hidden units per layer. All layers are fully connected. Ex. [64, 32] means first layer has 64 nodes and second one has 32.\n",
    "dnn_clf = tf.contrib.learn.DNNClassifier(hidden_units=[300,100], n_classes=10,\n",
    "                                         feature_columns=feature_cols, config=config)\n",
    "dnn_clf = tf.contrib.learn.SKCompat(dnn_clf) # if TensorFlow >= 1.1\n",
    "dnn_clf.fit(X_train, y_train, batch_size=50, steps=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can use the sklearn version of metrics\n",
    "\n",
    "from sklearn import metrics\n",
    "y_pred = dnn_clf.predict(X_test)\n",
    "#This calculates the accuracy.\n",
    "print(\"Accuracy score: \", metrics.accuracy_score(y_test,  y_pred['classes']) )\n",
    "\n",
    "#Log Loss is a way of score classes probabilsitically \n",
    "print(\"Logloss: \",metrics.log_loss(y_test, y_pred['probabilities']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow\n",
    "- Direct access to Python API for Tensorflow will give more flexibility \n",
    "- Like earlier, we will define the structure and then run the session. \n",
    "- set placeholders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "n_inputs = 28*28  # MNIST\n",
    "n_hidden1 = 300   # hidden units in first layer.\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10    # Classes of output variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Placehoder\n",
    "reset_graph()\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron_layer(X, n_neurons, name, activation=None):\n",
    "    with tf.name_scope(name):\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        stddev = 2 / np.sqrt(n_inputs)\n",
    "        init = tf.truncated_normal((n_inputs, n_neurons), stddev=stddev)\n",
    "        W = tf.Variable(init, name=\"kernel\")\n",
    "        b = tf.Variable(tf.zeros([n_neurons]), name=\"bias\")\n",
    "        Z = tf.matmul(X, W) + b\n",
    "        if activation is not None:\n",
    "            return activation(Z)\n",
    "        else:\n",
    "            return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = neuron_layer(X, n_hidden1, name=\"hidden1\", activation=tf.nn.relu)\n",
    "    hidden2 = neuron_layer(hidden1, n_hidden2, name=\"hidden2\", activation=tf.nn.relu)\n",
    "    logits = neuron_layer(hidden2, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Analysis over 40 Epocs\n",
    "- 40 passes through entire dataset.\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoc: 0 Train accuracy: 0.94 Test accuracy: 0.9084\n",
      "Epoc: 1 Train accuracy: 0.94 Test accuracy: 0.9289\n",
      "Epoc: 2 Train accuracy: 0.96 Test accuracy: 0.9401\n",
      "Epoc: 3 Train accuracy: 0.94 Test accuracy: 0.946\n",
      "Epoc: 4 Train accuracy: 0.96 Test accuracy: 0.9501\n",
      "Epoc: 5 Train accuracy: 0.94 Test accuracy: 0.9546\n",
      "Epoc: 6 Train accuracy: 0.94 Test accuracy: 0.957\n",
      "Epoc: 7 Train accuracy: 0.98 Test accuracy: 0.9582\n",
      "Epoc: 8 Train accuracy: 0.98 Test accuracy: 0.9613\n",
      "Epoc: 9 Train accuracy: 0.96 Test accuracy: 0.9649\n",
      "Epoc: 10 Train accuracy: 0.96 Test accuracy: 0.9661\n",
      "Epoc: 11 Train accuracy: 0.98 Test accuracy: 0.9662\n",
      "Epoc: 12 Train accuracy: 0.94 Test accuracy: 0.9679\n",
      "Epoc: 13 Train accuracy: 1.0 Test accuracy: 0.9693\n",
      "Epoc: 14 Train accuracy: 0.96 Test accuracy: 0.969\n",
      "Epoc: 15 Train accuracy: 1.0 Test accuracy: 0.9703\n",
      "Epoc: 16 Train accuracy: 1.0 Test accuracy: 0.9717\n",
      "Epoc: 17 Train accuracy: 1.0 Test accuracy: 0.9718\n",
      "Epoc: 18 Train accuracy: 0.98 Test accuracy: 0.9718\n",
      "Epoc: 19 Train accuracy: 1.0 Test accuracy: 0.9736\n",
      "Epoc: 20 Train accuracy: 1.0 Test accuracy: 0.9736\n",
      "Epoc: 21 Train accuracy: 0.98 Test accuracy: 0.9735\n",
      "Epoc: 22 Train accuracy: 1.0 Test accuracy: 0.9736\n",
      "Epoc: 23 Train accuracy: 1.0 Test accuracy: 0.9745\n",
      "Epoc: 24 Train accuracy: 1.0 Test accuracy: 0.9758\n",
      "Epoc: 25 Train accuracy: 1.0 Test accuracy: 0.9758\n",
      "Epoc: 26 Train accuracy: 1.0 Test accuracy: 0.9756\n",
      "Epoc: 27 Train accuracy: 1.0 Test accuracy: 0.9757\n",
      "Epoc: 28 Train accuracy: 1.0 Test accuracy: 0.977\n",
      "Epoc: 29 Train accuracy: 0.98 Test accuracy: 0.9762\n",
      "Epoc: 30 Train accuracy: 1.0 Test accuracy: 0.9773\n",
      "Epoc: 31 Train accuracy: 1.0 Test accuracy: 0.977\n",
      "Epoc: 32 Train accuracy: 1.0 Test accuracy: 0.9769\n",
      "Epoc: 33 Train accuracy: 0.98 Test accuracy: 0.9773\n",
      "Epoc: 34 Train accuracy: 0.98 Test accuracy: 0.9758\n",
      "Epoc: 35 Train accuracy: 0.98 Test accuracy: 0.978\n",
      "Epoc: 36 Train accuracy: 0.98 Test accuracy: 0.9787\n",
      "Epoc: 37 Train accuracy: 0.98 Test accuracy: 0.9779\n",
      "Epoc: 38 Train accuracy: 1.0 Test accuracy: 0.9785\n",
      "Epoc: 39 Train accuracy: 1.0 Test accuracy: 0.978\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 40\n",
    "batch_size = 50\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                            y: mnist.test.labels})\n",
    "        print(\"Epoc:\", epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\") # or better, use save_path\n",
    "    X_new_scaled = mnist.test.images[:20]\n",
    "    Z = logits.eval(feed_dict={X: X_new_scaled})\n",
    "    y_pred = np.argmax(Z, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Predicted classes:\", y_pred)\n",
    "print(\"Actual classes:   \", mnist.test.labels[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = \"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `dense()` instead of `neuron_layer()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the book uses `tensorflow.contrib.layers.fully_connected()` rather than `tf.layers.dense()` (which did not exist when this chapter was written). It is now preferable to use `tf.layers.dense()`, because anything in the contrib module may change or be deleted without notice. The `dense()` function is almost identical to the `fully_connected()` function, except for a few minor differences:\n",
    "* several parameters are renamed: `scope` becomes `name`, `activation_fn` becomes `activation` (and similarly the `_fn` suffix is removed from other parameters such as `normalizer_fn`), `weights_initializer` becomes `kernel_initializer`, etc.\n",
    "* the default `activation` is now `None` rather than `tf.nn.relu`.\n",
    "* a few more differences are presented in chapter 11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 28*28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\",\n",
    "                              activation=tf.nn.relu)\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\",\n",
    "                              activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "n_batches = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_graph(tf.get_default_graph())"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "nav_menu": {
   "height": "264px",
   "width": "369px"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
