{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"03-pytorch_iris.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"8qLm7tWHtSgn","colab_type":"text"},"cell_type":"markdown","source":["<center>[![AnalyticsDojo](https://raw.githubusercontent.com/rpi-techfundamentals/fall2018-materials/master/fig/final-logo.png)](http://rpi.analyticsdojo.com)\n","<h1>Revisiting IRIS with PyTorch</h1></center>\n","<center><h3><a href = 'http://rpi.analyticsdojo.com'>rpi.analyticsdojo.com</a></h3></center>\n"]},{"metadata":{"id":"aTIZ9QhHtSgs","colab_type":"code","colab":{}},"cell_type":"code","source":["!pip install torch torchvision"],"execution_count":0,"outputs":[]},{"metadata":{"id":"iz0-ym9QtShI","colab_type":"text"},"cell_type":"markdown","source":[""]},{"metadata":{"id":"CZySUCLOtShL","colab_type":"code","colab":{}},"cell_type":"code","source":["#Let's get rid of some imports\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","import numpy as np\n","#Define the model \n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F"],"execution_count":0,"outputs":[]},{"metadata":{"id":"zn_t2XgDtShS","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.datasets import load_iris\n","#Iris is available from the sklearn package\n","iris = load_iris()\n","X, y = iris.data, iris.target\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"63ca_uuWtShe","colab_type":"text"},"cell_type":"markdown","source":["---"]},{"metadata":{"id":"yNP12NKmtShg","colab_type":"text"},"cell_type":"markdown","source":["### Shuffled with Stratified Splitting\n","\n","- Especially for relatively small datasets, it's better to stratify the split.  \n","- Stratification means that we maintain the original class proportion of the dataset in the test and training sets. \n","- For example, after we randomly split the dataset as shown in the previous code example, we have the following class proportions in percent:"]},{"metadata":{"id":"B6cBRKqUtShi","colab_type":"text"},"cell_type":"markdown","source":["So, in order to stratify the split, we can pass the label array as an additional option to the `train_test_split` function:"]},{"metadata":{"id":"P2G9Ti29tShj","colab_type":"code","colab":{}},"cell_type":"code","source":["#Import Module\n","from sklearn.model_selection import train_test_split\n","train_X, val_X, train_y, val_y = train_test_split(X, y, \n","                                                    train_size=0.8,\n","                                                    test_size=0.2,\n","                                                    random_state=123,\n","                                                    stratify=y)\n","\n","print('All:', np.bincount(y) / float(len(y)) * 100.0)\n","print('Training:', np.bincount(train_y) / float(len(train_y)) * 100.0)\n","print('Validation:', np.bincount(val_y) / float(len(val_y)) * 100.0)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7Y-a_vY0x61h","colab_type":"code","colab":{}},"cell_type":"code","source":["train_y"],"execution_count":0,"outputs":[]},{"metadata":{"id":"lAHmfATZtShr","colab_type":"code","colab":{}},"cell_type":"code","source":["#This gives the number of columns, used below. \n","train_X.shape[1]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xNptEQEN-V4y","colab_type":"code","colab":{}},"cell_type":"code","source":["#This gives the number of classes, used below. \n","len(np.unique(train_y))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"RJPUnL0w9YZn","colab_type":"code","colab":{}},"cell_type":"code","source":["#Define training hyperprameters.\n","batch_size = 60\n","num_epochs = 500\n","learning_rate = 0.01\n","size_hidden= 100\n","\n","#Calculate some other hyperparameters based on data.  \n","batch_no = len(train_X) // batch_size  #batches\n","cols=train_X.shape[1] #Number of columns in input matrix\n","classes= len(np.unique(train_y))\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"OC8DC-UutShz","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","# Assume that we are on a CUDA machine, then this should print a CUDA device:\n","print(\"Executing the model on :\",device)\n","\n","class Net(nn.Module):\n","    def __init__(self,cols,size_hidden,classes):\n","        super(Net, self).__init__()\n","        #Note that 17 is the number of columns in the input matrix. \n","        self.fc1 = nn.Linear(cols, size_hidden)\n","        #variety of # possible for hidden layer size is arbitrary, but needs to be consistent across layers.  3 is the number of classes in the output (died/survived)\n","        self.fc2 = nn.Linear(size_hidden, classes)\n","        \n","    def forward(self, x):\n","        x = self.fc1(x)\n","        x = F.dropout(x, p=0.1)\n","        x = F.relu(x)\n","        x = self.fc2(x)\n","        return F.softmax(x, dim=1)\n","    \n","net = Net(cols, size_hidden, classes)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"iOSSOIcU_YXw","colab_type":"text"},"cell_type":"markdown","source":["See this on [Adam optimizer](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/).\n","\n","[Cross Entropy Loss Function](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html)\n","\n"]},{"metadata":{"id":"LfbvR2BsCoHV","colab_type":"code","colab":{}},"cell_type":"code","source":["#Adam is a specific flavor of gradient decent which is typically better\n","optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n","criterion = nn.CrossEntropyLoss()\n","\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Zd02kBpuvGLY","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.utils import shuffle\n","from torch.autograd import Variable\n","running_loss = 0.0\n","for epoch in range(num_epochs):\n","    #Shuffle just mixes up the dataset between epocs\n","    train_X, train_y = shuffle(train_X, train_y)\n","    # Mini batch learning\n","    for i in range(batch_no):\n","        start = i * batch_size\n","        end = start + batch_size\n","        inputs = Variable(torch.FloatTensor(train_X[start:end]))\n","        labels = Variable(torch.LongTensor(train_y[start:end]))\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # forward + backward + optimize\n","        outputs = net(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        # print statistics\n","        running_loss += loss.item()\n","        \n","    print('Epoch {}'.format(epoch+1), \"loss: \",running_loss)\n","    running_loss = 0.0\n","\n","        "],"execution_count":0,"outputs":[]},{"metadata":{"id":"IyCeZouHvvO0","colab_type":"code","colab":{}},"cell_type":"code","source":["import pandas as pd\n","#This is a little bit tricky to get the resulting prediction.  \n","def calculate_accuracy(x,y=[]):\n","    \"\"\"\n","    This function will return the accuracy if passed x and y or return predictions if just passed x. \n","    \"\"\"\n","    # Evaluate the model with the test set. \n","    X = Variable(torch.FloatTensor(x))  \n","    result = net(X) #This outputs the probability for each class.\n","    _, labels = torch.max(result.data, 1)\n","    if len(y) != 0:\n","        num_right = np.sum(labels.data.numpy() == y)\n","        print('Accuracy {:.2f}'.format(num_right / len(y)), \"for a total of \", len(y), \"records\")\n","        return pd.DataFrame(data= {'actual': y, 'predicted': labels.data.numpy()})\n","    else:\n","        print(\"returning predictions\")\n","        return labels.data.numpy()\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"93CNVTglwCFr","colab_type":"code","colab":{}},"cell_type":"code","source":["result1=calculate_accuracy(train_X,train_y)\n","result2=calculate_accuracy(val_X,val_y)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dkcDA3DAtSh8","colab_type":"text"},"cell_type":"markdown","source":["---"]},{"metadata":{"id":"deCNeoLktSh-","colab_type":"text"},"cell_type":"markdown","source":["### Comparison: Prediction using Simple Nearest Neighbor Classifier \n","- By evaluating our classifier performance on data that has been seen during training, we could get false confidence in the predictive power of our model. \n","- In the worst case, it may simply memorize the training samples but completely fails classifying new, similar samples -- we really don't want to put such a system into production!\n","- Instead of using the same dataset for training and testing (this is called \"resubstitution evaluation\"), it is much much better to use a train/test split in order to estimate how well your trained model is doing on new data."]},{"metadata":{"id":"S829OCottSh_","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.neighbors import KNeighborsClassifier\n","#This creates a model object.\n","classifier = KNeighborsClassifier()\n","#This fits the model object to the data.\n","classifier.fit(train_X, train_y)\n","#This creates the prediction. \n","pred_y = classifier.predict(val_X)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"E7HwZfB3tSiC","colab_type":"text"},"cell_type":"markdown","source":["### Scoring \n","- We can manually calculate the accuracy as we have done before. \n","- `metrics.accuracy_score` is passed the target value and the predicted value.Model objects also built in scoring functions.\n","- Can also us a `classifier.score` component built into the model. "]},{"metadata":{"id":"OojwVAtPtSiD","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn import metrics\n","\n","#This calculates the accuracy.\n","print(\"Classifier score: \", classifier.score(train_X, train_y) )\n","print(\"Classifier score: \", classifier.score(val_X, val_y) )\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ncFtNf0GtSik","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}