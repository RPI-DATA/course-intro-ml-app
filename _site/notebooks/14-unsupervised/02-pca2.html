<!DOCTYPE html>
<html lang="en">
  

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width,minimum-scale=1">

  <title>PCA Alt</title>
  <meta name="description" content="">

  <link rel="canonical" href="http://localhost:4000//notebooks/14-unsupervised/02-pca2.html">
  <link rel="alternate" type="application/rss+xml" title="MGMT6560 Fall 19" href="http://localhost:4000//feed.xml">

  <meta property="og:url"         content="http://localhost:4000//notebooks/14-unsupervised/02-pca2.html" />
<meta property="og:type"        content="article" />
<meta property="og:title"       content="PCA Alt" />
<meta property="og:description" content="" />
<meta property="og:image"       content="" />


  <script type="application/ld+json">
  {
  "@context": "http://schema.org",
  "@type": "NewsArticle",
  "mainEntityOfPage":
    "http://localhost:4000//notebooks/14-unsupervised/02-pca2.html",
  "headline":
    "PCA Alt",
  "datePublished":
    "2019-11-30T22:01:55-05:00",
  "dateModified":
    "2019-11-30T22:01:55-05:00",
  "description":
    "",
  "author": {
    "@type": "Person",
    "name": "Jason Kuruzovich"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Data 100 at UC Berkeley",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:4000/",
      "width": 60,
      "height": 60
    }
  },
  "image": {
    "@type": "ImageObject",
    "url": "http://localhost:4000/",
    "height": 60,
    "width": 60
  }
}

  </script>
  <link rel="stylesheet" href="/assets/css/styles.css">
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css ">
  <link rel="apple-touch-icon" sizes="57x57" href="/apple-touch-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="/apple-touch-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/apple-touch-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/apple-touch-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/apple-touch-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/apple-touch-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/apple-touch-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/apple-touch-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon-180x180.png">

  <!-- <link rel="manifest" href="/manifest.json"> -->
  <!-- <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#efae0a"> -->
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="msapplication-TileImage" content="/mstile-144x144.png">
  <meta name="theme-color" content="#233947">

  <!-- Favicon -->
  <link rel="shortcut icon" type="image/x-icon" href="/images/logo/favicon.png">

  <!-- MathJax Config -->
  <!-- Allow inline math using $ and automatically break long math lines -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true,
        processEnvironments: true
    },
    CommonHTML: {
        linebreaks: {
            automatic: true,
        },
    },
});
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_CHTML' async></script>

  <!-- DOM updating function -->
  <script>
const runWhenDOMLoaded = cb => {
  if (document.readyState != 'loading') {
    cb()
  } else if (document.addEventListener) {
    document.addEventListener('DOMContentLoaded', cb)
  } else {
    document.attachEvent('onreadystatechange', function() {
      if (document.readyState == 'complete') cb()
    })
  }
}

// Helper function to init things quickly
initFunction = function(myfunc) {
  runWhenDOMLoaded(myfunc);
  document.addEventListener('turbolinks:load', myfunc);
};
</script>

  <!-- Define some javascript variables that will be useful in other javascript -->
  <script>
    const site_basename = '/';
  </script>

  <!-- Add AnchorJS to let headers be linked -->
  <script src="/assets/js/anchor.min.js"  type="text/javascript"></script>
  <script>

initFunction(function () {
    anchors.add("main h1, main h2, main h3, main h4")
});

</script>

  <!-- Include Turbolinks to make page loads fast -->
  <!-- https://github.com/turbolinks/turbolinks -->
  <script src="/assets/js/turbolinks.js" async></script>
  <meta name="turbolinks-cache-control" content="no-cache">

  <!-- Load nbinteract for widgets -->
  

  <!-- Load Thebelab for interactive widgets -->
  <!-- Include Thebelab for interactive code if it's enabled -->



  <!-- Load the auto-generating TOC -->
  <script src="/assets/js/tocbot.min.js"  type="text/javascript"></script>
  <script>
var initToc = function () {
  tocbot.init({
    tocSelector: 'nav.onthispage',
    contentSelector: '.c-textbook__content',
    headingSelector: 'h2, h3',
    orderedList: false,
    collapseDepth: 6,
    listClass: 'toc__menu',
    activeListItemClass: "",  // Not using
    activeLinkClass: "", // Not using
  });
  tocbot.refresh();
}
initFunction(initToc);
</script>

  <!-- Google analytics -->
  <script src="/assets/js/ga.js" async></script>

  <!-- Clipboard copy button -->
  <script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js" async></script>

  <!-- Load JS that depends on site variables -->
  <script>
/**
 * Set up copy/paste for code blocks
 */
const codeCellId = index => `codecell${index}`

const clipboardButton = id =>
  `<a id="copy-button-${id}" class="btn copybtn o-tooltip--left" data-tooltip="Copy" data-clipboard-target="#${id}">
    <img src="/assets/images/copy-button.svg" alt="Copy to clipboard">
  </a>`

// Clears selected text since ClipboardJS will select the text when copying
const clearSelection = () => {
  if (window.getSelection) {
    window.getSelection().removeAllRanges()
  } else if (document.selection) {
    document.selection.empty()
  }
}

// Changes tooltip text for two seconds, then changes it back
const temporarilyChangeTooltip = (el, newText) => {
  const oldText = el.getAttribute('data-tooltip')
  el.setAttribute('data-tooltip', newText)
  setTimeout(() => el.setAttribute('data-tooltip', oldText), 2000)
}

const addCopyButtonToCodeCells = () => {
  // If ClipboardJS hasn't loaded, wait a bit and try again. This
  // happens because we load ClipboardJS asynchronously.
  if (window.ClipboardJS === undefined) {
    setTimeout(addCopyButtonToCodeCells, 250)
    return
  }

  const codeCells = document.querySelectorAll('div.c-textbook__content > div.highlighter-rouge > div.highlight > pre, div.input_area pre')
  codeCells.forEach((codeCell, index) => {
    const id = codeCellId(index)
    codeCell.setAttribute('id', id)
    if (document.getElementById("copy-button" + id) == null) {
      codeCell.insertAdjacentHTML('afterend', clipboardButton(id));
    }
  })

  const clipboard = new ClipboardJS('.copybtn')
  clipboard.on('success', event => {
    clearSelection()
    temporarilyChangeTooltip(event.trigger, 'Copied!')
  })

  clipboard.on('error', event => {
    temporarilyChangeTooltip(event.trigger, 'Failed to copy')
  })

  // Get rid of clipboard before the next page visit to avoid memory leak
  document.addEventListener('turbolinks:before-visit', () =>
    clipboard.destroy()
  )
}

initFunction(addCopyButtonToCodeCells);
</script>


  <!-- Hide cell code -->
  <script>
    /**
    Add buttons to hide code cells
    */


    var setCodeCellVisibility = function (inputField, kind) {
        // Update the image and class for hidden
        var id = inputField.getAttribute('data-id');
        var codeCell = document.querySelector(`#${id} div.highlight`);

        if (kind === "visible") {
            codeCell.classList.remove('hidden');
            inputField.checked = true;
        } else {
            codeCell.classList.add('hidden');
            inputField.checked = false;
        }
    }

    var toggleCodeCellVisibility = function (event) {
        // The label is clicked, and now we decide what to do based on the input field's clicked status
        if (event.target.tagName === "LABEL") {
            var inputField = event.target.previousElementSibling;
        } else {
            // It is the span inside the target
            var inputField = event.target.parentElement.previousElementSibling;
        }

        if (inputField.checked === true) {
            setCodeCellVisibility(inputField, "visible");
        } else {
            setCodeCellVisibility(inputField, "hidden");
        }
    }


    // Button constructor
    const hideCodeButton = id => `<input class="hidebtn" type="checkbox" id="hidebtn${id}" data-id="${id}"><label title="Toggle cell" for="hidebtn${id}" class="plusminus"><span class="pm_h"></span><span class="pm_v"></span></label>`

    var addHideButton = function () {
        // If a hide button is already added, don't add another
        if (document.querySelector('div.hidecode input') !== null) {
            return;
        }

        // Find the input cells and add a hide button
        document.querySelectorAll('div.input_area').forEach(function (item, index) {
            if (!item.classList.contains("hidecode")) {
                // Skip the cell if it doesn't have a hidecode class
                return;
            }

            const id = codeCellId(index)
            item.setAttribute('id', id);
            // Insert the button just inside the end of the next div
            item.querySelector('div').insertAdjacentHTML('beforeend', hideCodeButton(id))

            // Set up the visibility toggle
            hideLink = document.querySelector(`#${id} div.highlight + input + label`);
            hideLink.addEventListener('click', toggleCodeCellVisibility)
        });
    }


    // Initialize the hide buttos
    var initHiddenCells = function () {
        // Add hide buttons to the cells
        addHideButton();

        // Toggle the code cells that should be hidden
        document.querySelectorAll('div.hidecode input').forEach(function (item) {
            setCodeCellVisibility(item, 'hidden');
            item.checked = true;
        })
    }

    initFunction(initHiddenCells);

</script>

  <!-- Load custom website scripts -->
  <script src="/assets/js/scripts.js" async></script>

  <!-- Load custom user CSS and JS  -->
  <script src="/assets/custom/custom.js" async></script>
  <link rel="stylesheet" href="/assets/custom/custom.css">

  <!-- Update interact links w/ REST param, is defined in includes so we can use templates -->
  

  <!-- Lunr search code - will only be executed on the /search page -->
  <script src="/assets/js/lunr/lunr.min.js" type="text/javascript"></script>
  <script>var initQuery = function() {
  // See if we have a search box
  var searchInput = document.querySelector('input#lunr_search');
  if (searchInput === null) {
    return;
  }

  // Function to parse our lunr cache
  var idx = lunr(function () {
    this.field('title')
    this.field('excerpt')
    this.field('categories')
    this.field('tags')
    this.ref('id')

    this.pipeline.remove(lunr.trimmer)

    for (var item in store) {
      this.add({
        title: store[item].title,
        excerpt: store[item].excerpt,
        categories: store[item].categories,
        tags: store[item].tags,
        id: item
      })
    }
  });

  // Run search upon keyup
  searchInput.addEventListener('keyup', function () {
    var resultdiv = document.querySelector('#results');
    var query = document.querySelector("input#lunr_search").value.toLowerCase();
    var result =
      idx.query(function (q) {
        query.split(lunr.tokenizer.separator).forEach(function (term) {
          q.term(term, { boost: 100 })
          if(query.lastIndexOf(" ") != query.length-1){
            q.term(term, {  usePipeline: false, wildcard: lunr.Query.wildcard.TRAILING, boost: 10 })
          }
          if (term != ""){
            q.term(term, {  usePipeline: false, editDistance: 1, boost: 1 })
          }
        })
      });

      // Empty the results div
      while (resultdiv.firstChild) {
        resultdiv.removeChild(resultdiv.firstChild);
      }

    resultdiv.insertAdjacentHTML('afterbegin', '<p class="results__found">'+result.length+' Result(s) found</p>');
    for (var item in result) {
      var ref = result[item].ref;
      if(store[ref].teaser){
        var searchitem =
          '<div class="list__item">'+
            '<article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">'+
              '<h2 class="archive__item-title" itemprop="headline">'+
                '<a href="'+store[ref].url+'" rel="permalink">'+store[ref].title+'</a>'+
              '</h2>'+
              '<div class="archive__item-teaser">'+
                '<img src="'+store[ref].teaser+'" alt="">'+
              '</div>'+
              '<p class="archive__item-excerpt" itemprop="description">'+store[ref].excerpt.split(" ").splice(0,20).join(" ")+'...</p>'+
            '</article>'+
          '</div>';
      }
      else{
    	  var searchitem =
          '<div class="list__item">'+
            '<article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">'+
              '<h2 class="archive__item-title" itemprop="headline">'+
                '<a href="'+store[ref].url+'" rel="permalink">'+store[ref].title+'</a>'+
              '</h2>'+
              '<p class="archive__item-excerpt" itemprop="description">'+store[ref].excerpt.split(" ").splice(0,20).join(" ")+'...</p>'+
            '</article>'+
          '</div>';
      }
      resultdiv.insertAdjacentHTML('beforeend', searchitem);
    }
  });
};

initFunction(initQuery);
</script>
</head>

  <body>
    <!-- .js-show-sidebar shows sidebar by default -->
    <div id="js-textbook" class="c-textbook js-show-sidebar">
      



<nav id="js-sidebar" class="c-textbook__sidebar">
  <a href="https://rpi.analyticsdojo.com"><img src="/images/logo/rpi.png" class="textbook_logo" id="sidebar-logo" data-turbolinks-permanent/></a>
  <h2 class="c-sidebar__title">MGMT6560 Fall 19</h2>
  <ul class="c-sidebar__chapters">
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/index.html"
        >
          
          Home
        </a>

        
      </li>

      
    
      
      
        <li class="c-sidebar__chapter"><a class="c-sidebar__entry" href="/search.html">Search</a></li>
        
      
      
        <li class="c-sidebar__divider"></li>
        
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/sessions/index.html"
        >
          
          Schedule
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections u-hidden-visually">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/sessions/readings.html"
                >
                  
                  All Readings
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/sessions/session1.html"
                >
                  
                  Session 1
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/sessions/session2.html"
                >
                  
                  Session 2
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/sessions/session3.html"
                >
                  
                  Session 3
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/sessions/session4.html"
                >
                  
                  Session 4
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/sessions/session5.html"
                >
                  
                  Session 5
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/sessions/session6.html"
                >
                  
                  Session 6
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/sessions/session7.html"
                >
                  
                  Session 7
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/sessions/session8.html"
                >
                  
                  Session 8
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/sessions/session9.html"
                >
                  
                  Session 9
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/sessions/session10.html"
                >
                  
                  Session 10
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/sessions/session11.html"
                >
                  
                  Session 11
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/sessions/session12.html"
                >
                  
                  Session 12
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/sessions/session13.html"
                >
                  
                  Session 13
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/sessions/session14.html"
                >
                  
                  Session 14
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/sessions/session15.html"
                >
                  
                  Session 15
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/sessions/session16.html"
                >
                  
                  Session 16
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/sessions/session17.html"
                >
                  
                  Session 17
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/sessions/session18.html"
                >
                  
                  Session 18
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/sessions/session19.html"
                >
                  
                  Session 19
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/sessions/session20.html"
                >
                  
                  Session 20
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/sessions/session21.html"
                >
                  
                  Session 21
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/sessions/session22.html"
                >
                  
                  Session 22
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/sessions/session23.html"
                >
                  
                  Session 23
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/sessions/session24.html"
                >
                  
                  Session 24
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/sessions/session25.html"
                >
                  
                  Session 25
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/sessions/session26.html"
                >
                  
                  Session 26
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/sessions/session27.html"
                >
                  
                  Session 27
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/sessions/session28.html"
                >
                  
                  Session 28
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/sessions/session29.html"
                >
                  
                  Session 29
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/notebooks/index.html"
        >
          
          Notebooks
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections ">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/notebooks/02-intro-python/01-intro-python-overview.html"
                >
                  
                  Python Overview
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/notebooks/02-intro-python/02-intro-python-datastructures.html"
                >
                  
                  Basic Data Structures
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/notebooks/02-intro-python/03-intro-python-numpy.html"
                >
                  
                  Numpy
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/notebooks/02-intro-python/04-intro-python-pandas.html"
                >
                  
                  Pandas
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/notebooks/04-python/01-intro-python-conditionals-loops.html"
                >
                  
                  Conditional-Loops
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/notebooks/04-python/02-intro-python-functions.html"
                >
                  
                  Functions
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/notebooks/04-python/03-intro-python-null-values.html"
                >
                  
                  Null Values
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/notebooks/04-python/04-intro-python-groupby.html"
                >
                  
                  Groupby and Pivot Tables
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/notebooks/04-python/05-intro-kaggle-baseline.html"
                >
                  
                  Kaggle Baseline
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/notebooks/06-viz-api-scraper/01-intro-api-twitter.html"
                >
                  
                  Twitter
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/notebooks/06-viz-api-scraper/02-intro-python-webmining.html"
                >
                  
                  Web Mining
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/notebooks/06-viz-api-scraper/03-visualization-python-seaborn.html"
                >
                  
                  Visualizations - Seaborn
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/notebooks/06-viz-api-scraper/04-strings-regular-expressions.html"
                >
                  
                  Strings - Regular Expressions
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/notebooks/06-viz-api-scraper/05-features-dummies.html"
                >
                  
                  Feature Dummies
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/notebooks/06-viz-api-scraper/ALT-visualization-python-matplotlib.html"
                >
                  
                  Matplotlib
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/notebooks/08-intro-modeling/01-neural-networks.html"
                >
                  
                  The Simplest Neural Network with Numpy
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/notebooks/08-intro-modeling/02-train-test-split.html"
                >
                  
                  Train Test Split
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/notebooks/08-intro-modeling/03-intro-logistic-knn.html"
                >
                  
                  Introduction to Logistic Regression
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/notebooks/08-intro-modeling/04-knn.html"
                >
                  
                  K Nearest Neighbor
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/notebooks/assignments/05-starter.html"
                >
                  
                  Assignment 5
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/notebooks/10-intro-r/01-intro-r-overview.html"
                >
                  
                  Introduction to R
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/notebooks/10-intro-r/02-intro-r-localfile.html"
                >
                  
                  Local Files
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/notebooks/10-intro-r/03-intro-r-datastructures.html"
                >
                  
                  Data Structures
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/notebooks/10-intro-r/04-intro-r-dataframes.html"
                >
                  
                  Dataframes
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/notebooks/10-intro-r/05-intro-r-functions.html"
                >
                  
                  Functions
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/notebooks/10-intro-r/06-intro-r-conditionals-loops.html"
                >
                  
                  Conditional-Loops
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/notebooks/10-intro-r/07-intro-r-merge-agg-fun.html"
                >
                  
                  Aggregation and Merge
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/notebooks/10-intro-r/08-intro-r-tidyverse.html"
                >
                  
                  Tidyvere
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/notebooks/10-intro-r/09-titanic-intro.html"
                >
                  
                  Titanic
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/notebooks/12-intro-modeling-2/01-matrix-regression-gradient-decent-python.html"
                >
                  
                  Regression - Matrix
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/notebooks/12-intro-modeling-2/02-regression-boston-housing-python.html"
                >
                  
                  Boston Housing
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/notebooks/12-intro-modeling-2/03-ridge-lasso-python.html"
                >
                  
                  Ridge and Lasso
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/notebooks/12-intro-modeling-2/04-stats-models.html"
                >
                  
                  Stats Models
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/notebooks/14-unsupervised/01-introduction-pca.html"
                >
                  
                  PCA
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry c-sidebar__entry--active"
                  href="/notebooks/14-unsupervised/02-pca2.html"
                >
                  
                  PCA Alt
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/notebooks/14-unsupervised/03-kmeans.html"
                >
                  
                  Cluster Analysis
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/notebooks/14-unsupervised/04-regression-feature-selection.html"
                >
                  
                  Feature Selection and Importance
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/notebooks/16-intro-nlp/01-titanic-features.html"
                >
                  
                  Titanic Feature Creation
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/notebooks/16-intro-nlp/02-corpus-simple.html"
                >
                  
                  Corpus Simple
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/notebooks/16-intro-nlp/03-scikit-learn-text.html"
                >
                  
                  Scikit Learn Text
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/notebooks/16-intro-nlp/04-what-cooking-python.html"
                >
                  
                  What's Cooking Python
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/notebooks/16-intro-nlp/05-bag-popcorn-bag-words.html"
                >
                  
                  Bag of Popcorn Bag of Words
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/notebooks/16-intro-nlp/06-sentiment.html"
                >
                  
                  Sentiment
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/notebooks/16-intro-nlp/02-intro-nlp.html"
                >
                  
                  Overview of NLP
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/notebooks/16-intro-nlp/07-fastai-imdb.html"
                >
                  
                  FAST.ai NLP
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/notebooks/18-big-data/01-intro-mapreduce.html"
                >
                  
                  Intoduction to MapReduce
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/notebooks/18-big-data/02-intro-spark.html"
                >
                  
                  Introduction to Spark
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/notebooks/18-intro-timeseries/01-time-series.html"
                >
                  
                  Introduction to Time Series
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/notebooks/18-intro-timeseries/02-forcasting-rossman.html"
                >
                  
                  Rossman Store Sales
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/notebooks/20-deep-learning1/01-neural-networks.html"
                >
                  
                  Neural Networks
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/notebooks/20-deep-learning1/02-tensor-tutorial.html"
                >
                  
                  Tensors
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/notebooks/20-deep-learning1/03-pytorch-iris.html"
                >
                  
                  Pytorch IRIS
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/notebooks/20-deep-learning1/04-covnet-tutorial.html"
                >
                  
                  Covnet
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/notebooks/20-deep-learning1/05-pytorch-mnist.html"
                >
                  
                  Pytorch Mnist
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/notebooks/20-deep-learning1/06-regression-bh-pytorch.html"
                >
                  
                  Regression
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/notebooks/20-deep-learning1/07-titanic-fastai.html"
                >
                  
                  Titanic FastAI
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/notebooks/20-deep-learning1/08-ludwig.html"
                >
                  
                  Ludwig
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/notebooks/20-deep-learning1/09-evaluation.html"
                >
                  
                  Evaluation
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/notebooks/24-tensorflow/10_neural_nets_with_keras.html"
                >
                  
                  TF-Keras
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/notebooks/24-tensorflow/11_training_deep_neural_networks.html"
                >
                  
                  TF-training
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/notebooks/24-tensorflow/13_loading_and_preprocessing_data.html"
                >
                  
                  TF-data
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/notebooks/24-tensorflow/14_deep_computer_vision_with_cnns.html"
                >
                  
                  TF-CNN
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/notebooks/24-tensorflow/15_processing_sequences_using_rnns_and_cnns.html"
                >
                  
                  TF-RNN
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/notebooks/24-tensorflow/16_nlp_with_rnns_and_attention.html"
                >
                  
                  TF-NLP
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/notebooks/24-tensorflow/17_autoencoders_and_gans.html"
                >
                  
                  TF-Autoencoder and Gan
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/assignments/index.html"
        >
          
          Assignments
        </a>

        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/grading.html"
        >
          
          Grading
        </a>

        
      </li>

      
    
      
      
        <li class="c-sidebar__divider"></li>
        
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="https://colab.research.google.com"
        >
          
          Google Colab
        </a>

        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="https://www.dropbox.com/sh/n34sld9qjxyc2xi/AADTrNLgPlu2FNVEhHG04Qqxa?dl=0"
        >
          
          Dropbox - Presentations/Files
        </a>

        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="https://github.com/RPI-DATA/course-intro-ml-app/tree/master/content"
        >
          
          Github - Class Content
        </a>

        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="https://github.com/orgs/rpi-intro-ml-app-fall-2019/"
        >
          
          Github - Assignments
        </a>

        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="https://piazza.com/class/jzeez8noxxl7eh"
        >
          
          Piazza (Section 2 Communications)
        </a>

        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="https://rpibsan2020.slack.com/"
        >
          
          Slack (Section 1 Communications)
        </a>

        
      </li>

      
    
  </ul>
  <p class="sidebar_footer">Powered by <a href="https://github.com/jupyter/jupyter-book">Jupyter Book</a></p>
</nav>

      
      <!-- Empty sidebar placeholder that we'll auto-fill with javascript -->
      <aside class="sidebar__right">
          <header><h4 class="nav__title"><i class="fa fa-list"></i>   On this page</h4></header>
          <nav class="onthispage">
          </nav>
      </aside>
      
      <main class="c-textbook__page" tabindex="-1">
          <div class="o-wrapper">
            <div class="c-sidebar-toggle">
  <!-- We show the sidebar by default so we use .is-active -->
  <button
    id="js-sidebar-toggle"
    class="hamburger hamburger--arrowalt is-active"
  >
    <span class="hamburger-box">
      <span class="hamburger-inner"></span>
    </span>
    <span class="c-sidebar-toggle__label">Toggle Sidebar</span>
  </button>
</div>

            
<div class="buttons">
<a href="/content/notebooks/14-unsupervised/02-pca2.ipynb" download>
<button id="interact-button-download" class="interact-button">Download</button>
</a>






</div>


            <div class="c-textbook__content">
              <!--BOOK_INFORMATION-->
<p><img align="left" style="padding-right:10px;" src="figures/PDSH-cover-small.png" /></p>

<p><em>This notebook contains an excerpt from the <a href="http://shop.oreilly.com/product/0636920034919.do">Python Data Science Handbook</a> by Jake VanderPlas; the content is available <a href="https://github.com/jakevdp/PythonDataScienceHandbook">on GitHub</a>.</em></p>

<p><em>The text is released under the <a href="https://creativecommons.org/licenses/by-nc-nd/3.0/us/legalcode">CC-BY-NC-ND license</a>, and code is released under the <a href="https://opensource.org/licenses/MIT">MIT license</a>. If you find this content useful, please consider supporting the work by <a href="http://shop.oreilly.com/product/0636920034919.do">buying the book</a>!</em></p>

<!--NAVIGATION-->
<p>&lt; <a href="05.08-Random-Forests.ipynb">In-Depth: Decision Trees and Random Forests</a> | <a href="Index.ipynb">Contents</a> | <a href="05.10-Manifold-Learning.ipynb">In-Depth: Manifold Learning</a> &gt;</p>

<p><a href="https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.09-Principal-Component-Analysis.ipynb"><img align="left" src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab" title="Open and Execute in Google Colaboratory" /></a></p>

<h1 id="in-depth-principal-component-analysis">In Depth: Principal Component Analysis</h1>

<p>Up until now, we have been looking in depth at supervised learning estimators: those estimators that predict labels based on labeled training data.
Here we begin looking at several unsupervised estimators, which can highlight interesting aspects of the data without reference to any known labels.</p>

<p>In this section, we explore what is perhaps one of the most broadly used of unsupervised algorithms, principal component analysis (PCA).
PCA is fundamentally a dimensionality reduction algorithm, but it can also be useful as a tool for visualization, for noise filtering, for feature extraction and engineering, and much more.
After a brief conceptual discussion of the PCA algorithm, we will see a couple examples of these further applications.</p>

<p>We begin with the standard imports:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span><span class="p">;</span> <span class="n">sns</span><span class="o">.</span><span class="nb">set</span><span class="p">()</span>

</code></pre></div>    </div>
  </div>

</div>

<h2 id="introducing-principal-component-analysis">Introducing Principal Component Analysis</h2>

<p>Principal component analysis is a fast and flexible unsupervised method for dimensionality reduction in data, which we saw briefly in <a href="05.02-Introducing-Scikit-Learn.ipynb">Introducing Scikit-Learn</a>.
Its behavior is easiest to visualize by looking at a two-dimensional dataset.
Consider the following 200 points:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">rng</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">rng</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">200</span><span class="p">))</span><span class="o">.</span><span class="n">T</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'equal'</span><span class="p">);</span>

</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/notebooks/14-unsupervised/02-pca2_6_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>By eye, it is clear that there is a nearly linear relationship between the x and y variables.
This is reminiscent of the linear regression data we explored in <a href="05.06-Linear-Regression.ipynb">In Depth: Linear Regression</a>, but the problem setting here is slightly different: rather than attempting to <em>predict</em> the y values from the x values, the unsupervised learning problem attempts to learn about the <em>relationship</em> between the x and y values.</p>

<p>In principal component analysis, this relationship is quantified by finding a list of the <em>principal axes</em> in the data, and using those axes to describe the dataset.
Using Scikit-Learns <code class="language-plaintext highlighter-rouge">PCA</code> estimator, we can compute this as follows:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="language-plaintext output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>PCA(copy=True, n_components=2, whiten=False)
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>The fit learns some quantities from the data, most importantly the components and explained variance:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">components_</span><span class="p">)</span>

</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">
      <div class="language-plaintext output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[[ 0.94446029  0.32862557]
 [ 0.32862557 -0.94446029]]
</code></pre></div>      </div>
    </div>
  </div>
</div>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_</span><span class="p">)</span>

</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">
      <div class="language-plaintext output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[ 0.75871884  0.01838551]
</code></pre></div>      </div>
    </div>
  </div>
</div>

<p>To see what these numbers mean, lets visualize them as vectors over the input data, using the components to define the direction of the vector, and the explained variance to define the squared-length of the vector:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">draw_vector</span><span class="p">(</span><span class="n">v0</span><span class="p">,</span> <span class="n">v1</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span> <span class="ow">or</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
    <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s">'-&gt;'</span><span class="p">,</span>
                    <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                    <span class="n">shrinkA</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">shrinkB</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s">''</span><span class="p">,</span> <span class="n">v1</span><span class="p">,</span> <span class="n">v0</span><span class="p">,</span> <span class="n">arrowprops</span><span class="o">=</span><span class="n">arrowprops</span><span class="p">)</span>

<span class="c1"># plot data
</span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="k">for</span> <span class="n">length</span><span class="p">,</span> <span class="n">vector</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_</span><span class="p">,</span> <span class="n">pca</span><span class="o">.</span><span class="n">components_</span><span class="p">):</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">vector</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">length</span><span class="p">)</span>
    <span class="n">draw_vector</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">mean_</span><span class="p">,</span> <span class="n">pca</span><span class="o">.</span><span class="n">mean_</span> <span class="o">+</span> <span class="n">v</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'equal'</span><span class="p">);</span>

</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/notebooks/14-unsupervised/02-pca2_13_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>These vectors represent the <em>principal axes</em> of the data, and the length of the vector is an indication of how important that axis is in describing the distribution of the datamore precisely, it is a measure of the variance of the data when projected onto that axis.
The projection of each data point onto the principal axes are the principal components of the data.</p>

<p>If we plot these principal components beside the original data, we see the plots shown here:</p>

<p><img src="figures/05.09-PCA-rotation.png" alt="" />
<a href="06.00-Figure-Code.ipynb#Principal-Components-Rotation">figure source in Appendix</a></p>

<p>This transformation from data axes to principal axes is an <em>affine transformation</em>, which basically means it is composed of a translation, rotation, and uniform scaling.</p>

<p>While this algorithm to find principal components may seem like just a mathematical curiosity, it turns out to have very far-reaching applications in the world of machine learning and data exploration.</p>

<h3 id="pca-as-dimensionality-reduction">PCA as dimensionality reduction</h3>

<p>Using PCA for dimensionality reduction involves zeroing out one or more of the smallest principal components, resulting in a lower-dimensional projection of the data that preserves the maximal data variance.</p>

<p>Here is an example of using PCA as a dimensionality reduction transform:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">X_pca</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"original shape:   "</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"transformed shape:"</span><span class="p">,</span> <span class="n">X_pca</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">
      <div class="language-plaintext output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>original shape:    (200, 2)
transformed shape: (200, 1)
</code></pre></div>      </div>
    </div>
  </div>
</div>

<p>The transformed data has been reduced to a single dimension.
To understand the effect of this dimensionality reduction, we can perform the inverse transform of this reduced data and plot it along with the original data:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X_new</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">X_pca</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_new</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_new</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'equal'</span><span class="p">);</span>

</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/notebooks/14-unsupervised/02-pca2_20_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>The light points are the original data, while the dark points are the projected version.
This makes clear what a PCA dimensionality reduction means: the information along the least important principal axis or axes is removed, leaving only the component(s) of the data with the highest variance.
The fraction of variance that is cut out (proportional to the spread of points about the line formed in this figure) is roughly a measure of how much information is discarded in this reduction of dimensionality.</p>

<p>This reduced-dimension dataset is in some senses good enough to encode the most important relationships between the points: despite reducing the dimension of the data by 50%, the overall relationship between the data points are mostly preserved.</p>

<h3 id="pca-for-visualization-hand-written-digits">PCA for visualization: Hand-written digits</h3>

<p>The usefulness of the dimensionality reduction may not be entirely apparent in only two dimensions, but becomes much more clear when looking at high-dimensional data.
To see this, lets take a quick look at the application of PCA to the digits data we saw in <a href="05.08-Random-Forests.ipynb">In-Depth: Decision Trees and Random Forests</a>.</p>

<p>We start by loading the data:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_digits</span>
<span class="n">digits</span> <span class="o">=</span> <span class="n">load_digits</span><span class="p">()</span>
<span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span>

</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="language-plaintext output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(1797, 64)
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>Recall that the data consists of 88 pixel images, meaning that they are 64-dimensional.
To gain some intuition into the relationships between these points, we can use PCA to project them to a more manageable number of dimensions, say two:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># project from 64 to 2 dimensions
</span><span class="n">projected</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">projected</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">
      <div class="language-plaintext output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(1797, 64)
(1797, 2)
</code></pre></div>      </div>
    </div>
  </div>
</div>

<p>We can now plot the first two principal components of each point to learn about the data:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">projected</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">projected</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span>
            <span class="n">c</span><span class="o">=</span><span class="n">digits</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s">'none'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
            <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s">'spectral'</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'component 1'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'component 2'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">();</span>

</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/notebooks/14-unsupervised/02-pca2_27_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>Recall what these components mean: the full data is a 64-dimensional point cloud, and these points are the projection of each data point along the directions with the largest variance.
Essentially, we have found the optimal stretch and rotation in 64-dimensional space that allows us to see the layout of the digits in two dimensions, and have done this in an unsupervised mannerthat is, without reference to the labels.</p>

<h3 id="what-do-the-components-mean">What do the components mean?</h3>

<p>We can go a bit further here, and begin to ask what the reduced dimensions <em>mean</em>.
This meaning can be understood in terms of combinations of basis vectors.
For example, each image in the training set is defined by a collection of 64 pixel values, which we will call the vector $x$:</p>

<script type="math/tex; mode=display">x = [x_1, x_2, x_3 \cdots x_{64}]</script>

<p>One way we can think about this is in terms of a pixel basis.
That is, to construct the image, we multiply each element of the vector by the pixel it describes, and then add the results together to build the image:</p>

<script type="math/tex; mode=display">{\rm image}(x) = x_1 \cdot{\rm (pixel~1)} + x_2 \cdot{\rm (pixel~2)} + x_3 \cdot{\rm (pixel~3)} \cdots x_{64} \cdot{\rm (pixel~64)}</script>

<p>One way we might imagine reducing the dimension of this data is to zero out all but a few of these basis vectors.
For example, if we use only the first eight pixels, we get an eight-dimensional projection of the data, but it is not very reflective of the whole image: weve thrown out nearly 90% of the pixels!</p>

<p><img src="figures/05.09-digits-pixel-components.png" alt="" />
<a href="06.00-Figure-Code.ipynb#Digits-Pixel-Components">figure source in Appendix</a></p>

<p>The upper row of panels shows the individual pixels, and the lower row shows the cumulative contribution of these pixels to the construction of the image.
Using only eight of the pixel-basis components, we can only construct a small portion of the 64-pixel image.
Were we to continue this sequence and use all 64 pixels, we would recover the original image.</p>

<p>But the pixel-wise representation is not the only choice of basis. We can also use other basis functions, which each contain some pre-defined contribution from each pixel, and write something like</p>

<script type="math/tex; mode=display">image(x) = {\rm mean} + x_1 \cdot{\rm (basis~1)} + x_2 \cdot{\rm (basis~2)} + x_3 \cdot{\rm (basis~3)} \cdots</script>

<p>PCA can be thought of as a process of choosing optimal basis functions, such that adding together just the first few of them is enough to suitably reconstruct the bulk of the elements in the dataset.
The principal components, which act as the low-dimensional representation of our data, are simply the coefficients that multiply each of the elements in this series.
This figure shows a similar depiction of reconstructing this digit using the mean plus the first eight PCA basis functions:</p>

<p><img src="figures/05.09-digits-pca-components.png" alt="" />
<a href="06.00-Figure-Code.ipynb#Digits-PCA-Components">figure source in Appendix</a></p>

<p>Unlike the pixel basis, the PCA basis allows us to recover the salient features of the input image with just a mean plus eight components!
The amount of each pixel in each component is the corollary of the orientation of the vector in our two-dimensional example.
This is the sense in which PCA provides a low-dimensional representation of the data: it discovers a set of basis functions that are more efficient than the native pixel-basis of the input data.</p>

<h3 id="choosing-the-number-of-components">Choosing the number of components</h3>

<p>A vital part of using PCA in practice is the ability to estimate how many components are needed to describe the data.
This can be determined by looking at the cumulative <em>explained variance ratio</em> as a function of the number of components:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'number of components'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'cumulative explained variance'</span><span class="p">);</span>

</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/notebooks/14-unsupervised/02-pca2_36_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>This curve quantifies how much of the total, 64-dimensional variance is contained within the first $N$ components.
For example, we see that with the digits the first 10 components contain approximately 75% of the variance, while you need around 50 components to describe close to 100% of the variance.</p>

<p>Here we see that our two-dimensional projection loses a lot of information (as measured by the explained variance) and that wed need about 20 components to retain 90% of the variance.  Looking at this plot for a high-dimensional dataset can help you understand the level of redundancy present in multiple observations.</p>

<h2 id="pca-as-noise-filtering">PCA as Noise Filtering</h2>

<p>PCA can also be used as a filtering approach for noisy data.
The idea is this: any components with variance much larger than the effect of the noise should be relatively unaffected by the noise.
So if you reconstruct the data using just the largest subset of principal components, you should be preferentially keeping the signal and throwing out the noise.</p>

<p>Lets see how this looks with the digits data.
First we will plot several of the input noise-free data:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">plot_digits</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span>
                             <span class="n">subplot_kw</span><span class="o">=</span><span class="p">{</span><span class="s">'xticks'</span><span class="p">:[],</span> <span class="s">'yticks'</span><span class="p">:[]},</span>
                             <span class="n">gridspec_kw</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">hspace</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.1</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axes</span><span class="o">.</span><span class="n">flat</span><span class="p">):</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span>
                  <span class="n">cmap</span><span class="o">=</span><span class="s">'binary'</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s">'nearest'</span><span class="p">,</span>
                  <span class="n">clim</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">16</span><span class="p">))</span>
<span class="n">plot_digits</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>

</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/notebooks/14-unsupervised/02-pca2_39_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>Now lets add some random noise to create a noisy dataset, and re-plot it:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">noisy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">plot_digits</span><span class="p">(</span><span class="n">noisy</span><span class="p">)</span>

</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/notebooks/14-unsupervised/02-pca2_41_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>Its clear by eye that the images are noisy, and contain spurious pixels.
Lets train a PCA on the noisy data, requesting that the projection preserve 50% of the variance:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="mf">0.50</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">noisy</span><span class="p">)</span>
<span class="n">pca</span><span class="o">.</span><span class="n">n_components_</span>

</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="language-plaintext output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>12
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>Here 50% of the variance amounts to 12 principal components.
Now we compute these components, and then use the inverse of the transform to reconstruct the filtered digits:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">components</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">noisy</span><span class="p">)</span>
<span class="n">filtered</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">components</span><span class="p">)</span>
<span class="n">plot_digits</span><span class="p">(</span><span class="n">filtered</span><span class="p">)</span>

</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/notebooks/14-unsupervised/02-pca2_45_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>This signal preserving/noise filtering property makes PCA a very useful feature selection routinefor example, rather than training a classifier on very high-dimensional data, you might instead train the classifier on the lower-dimensional representation, which will automatically serve to filter out random noise in the inputs.</p>

<h2 id="example-eigenfaces">Example: Eigenfaces</h2>

<p>Earlier we explored an example of using a PCA projection as a feature selector for facial recognition with a support vector machine (see <a href="05.07-Support-Vector-Machines.ipynb">In-Depth: Support Vector Machines</a>).
Here we will take a look back and explore a bit more of what went into that.
Recall that we were using the Labeled Faces in the Wild dataset made available through Scikit-Learn:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_lfw_people</span>
<span class="n">faces</span> <span class="o">=</span> <span class="n">fetch_lfw_people</span><span class="p">(</span><span class="n">min_faces_per_person</span><span class="o">=</span><span class="mi">60</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">faces</span><span class="o">.</span><span class="n">target_names</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">faces</span><span class="o">.</span><span class="n">images</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">
      <div class="language-plaintext output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>['Ariel Sharon' 'Colin Powell' 'Donald Rumsfeld' 'George W Bush'
 'Gerhard Schroeder' 'Hugo Chavez' 'Junichiro Koizumi' 'Tony Blair']
(1348, 62, 47)
</code></pre></div>      </div>
    </div>
  </div>
</div>

<p>Lets take a look at the principal axes that span this dataset.
Because this is a large dataset, we will use <code class="language-plaintext highlighter-rouge">RandomizedPCA</code>it contains a randomized method to approximate the first $N$ principal components much more quickly than the standard <code class="language-plaintext highlighter-rouge">PCA</code> estimator, and thus is very useful for high-dimensional data (here, a dimensionality of nearly 3,000).
We will take a look at the first 150 components:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">RandomizedPCA</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">RandomizedPCA</span><span class="p">(</span><span class="mi">150</span><span class="p">)</span>
<span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">faces</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>

</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="language-plaintext output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>RandomizedPCA(copy=True, iterated_power=3, n_components=150,
       random_state=None, whiten=False)
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>In this case, it can be interesting to visualize the images associated with the first several principal components (these components are technically known as eigenvectors,
so these types of images are often called eigenfaces).
As you can see in this figure, they are as creepy as they sound:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span>
                         <span class="n">subplot_kw</span><span class="o">=</span><span class="p">{</span><span class="s">'xticks'</span><span class="p">:[],</span> <span class="s">'yticks'</span><span class="p">:[]},</span>
                         <span class="n">gridspec_kw</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">hspace</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.1</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axes</span><span class="o">.</span><span class="n">flat</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">components_</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">62</span><span class="p">,</span> <span class="mi">47</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'bone'</span><span class="p">)</span>

</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/notebooks/14-unsupervised/02-pca2_52_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>The results are very interesting, and give us insight into how the images vary: for example, the first few eigenfaces (from the top left) seem to be associated with the angle of lighting on the face, and later principal vectors seem to be picking out certain features, such as eyes, noses, and lips.
Lets take a look at the cumulative variance of these components to see how much of the data information the projection is preserving:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'number of components'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'cumulative explained variance'</span><span class="p">);</span>

</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/notebooks/14-unsupervised/02-pca2_54_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>We see that these 150 components account for just over 90% of the variance.
That would lead us to believe that using these 150 components, we would recover most of the essential characteristics of the data.
To make this more concrete, we can compare the input images with the images reconstructed from these 150 components:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Compute the components and projected faces
</span><span class="n">pca</span> <span class="o">=</span> <span class="n">RandomizedPCA</span><span class="p">(</span><span class="mi">150</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">faces</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
<span class="n">components</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">faces</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
<span class="n">projected</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">components</span><span class="p">)</span>

</code></pre></div>    </div>
  </div>

</div>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Plot the results
</span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">),</span>
                       <span class="n">subplot_kw</span><span class="o">=</span><span class="p">{</span><span class="s">'xticks'</span><span class="p">:[],</span> <span class="s">'yticks'</span><span class="p">:[]},</span>
                       <span class="n">gridspec_kw</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">hspace</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.1</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">faces</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">62</span><span class="p">,</span> <span class="mi">47</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'binary_r'</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">projected</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">62</span><span class="p">,</span> <span class="mi">47</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'binary_r'</span><span class="p">)</span>
    
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">'full-dim</span><span class="se">\n</span><span class="s">input'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">'150-dim</span><span class="se">\n</span><span class="s">reconstruction'</span><span class="p">);</span>

</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../../images/notebooks/14-unsupervised/02-pca2_57_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>The top row here shows the input images, while the bottom row shows the reconstruction of the images from just 150 of the ~3,000 initial features.
This visualization makes clear why the PCA feature selection used in <a href="05.07-Support-Vector-Machines.ipynb">In-Depth: Support Vector Machines</a> was so successful: although it reduces the dimensionality of the data by nearly a factor of 20, the projected images contain enough information that we might, by eye, recognize the individuals in the image.
What this means is that our classification algorithm needs to be trained on 150-dimensional data rather than 3,000-dimensional data, which depending on the particular algorithm we choose, can lead to a much more efficient classification.</p>

<h2 id="principal-component-analysis-summary">Principal Component Analysis Summary</h2>

<p>In this section we have discussed the use of principal component analysis for dimensionality reduction, for visualization of high-dimensional data, for noise filtering, and for feature selection within high-dimensional data.
Because of the versatility and interpretability of PCA, it has been shown to be effective in a wide variety of contexts and disciplines.
Given any high-dimensional dataset, I tend to start with PCA in order to visualize the relationship between points (as we did with the digits), to understand the main variance in the data (as we did with the eigenfaces), and to understand the intrinsic dimensionality (by plotting the explained variance ratio).
Certainly PCA is not useful for every high-dimensional dataset, but it offers a straightforward and efficient path to gaining insight into high-dimensional data.</p>

<p>PCAs main weakness is that it tends to be highly affected by outliers in the data.
For this reason, many robust variants of PCA have been developed, many of which act to iteratively discard data points that are poorly described by the initial components.
Scikit-Learn contains a couple interesting variants on PCA, including <code class="language-plaintext highlighter-rouge">RandomizedPCA</code> and <code class="language-plaintext highlighter-rouge">SparsePCA</code>, both also in the <code class="language-plaintext highlighter-rouge">sklearn.decomposition</code> submodule.
<code class="language-plaintext highlighter-rouge">RandomizedPCA</code>, which we saw earlier, uses a non-deterministic method to quickly approximate the first few principal components in very high-dimensional data, while <code class="language-plaintext highlighter-rouge">SparsePCA</code> introduces a regularization term (see <a href="05.06-Linear-Regression.ipynb">In Depth: Linear Regression</a>) that serves to enforce sparsity of the components.</p>

<p>In the following sections, we will look at other unsupervised learning methods that build on some of the ideas of PCA.</p>

<!--NAVIGATION-->
<p>&lt; <a href="05.08-Random-Forests.ipynb">In-Depth: Decision Trees and Random Forests</a> | <a href="Index.ipynb">Contents</a> | <a href="05.10-Manifold-Learning.ipynb">In-Depth: Manifold Learning</a> &gt;</p>

<p><a href="https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.09-Principal-Component-Analysis.ipynb"><img align="left" src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab" title="Open and Execute in Google Colaboratory" /></a></p>


              <nav class="c-page__nav">
  
    
    <a id="js-page__nav__prev" class="c-page__nav__prev" href="/notebooks/14-unsupervised/01-introduction-pca.html">
       <span class="u-margin-right-tiny"></span> PCA
    </a>
  

  
    
    <a id="js-page__nav__next" class="c-page__nav__next" href="/notebooks/14-unsupervised/03-kmeans.html">
      Cluster Analysis <span class="u-margin-right-tiny"></span> 
    </a>
  
</nav>

            </div>
          </div>
        </div>
      </main>
    </div>

  </body>
</html>
